[Learning to See](http://www.memo.tv/portfolio/learning-to-see/)

But it's XR

- [[Memo Akten]] #[[Human-AI Collaboration]]
    - http://www.memo.tv/works/learning-to-see/
    - # Learning to See (2017)
    - Part of the **Learning to See** series:
        - [Learning to See](http://www.memo.tv/portfolio/learning-to-see/)
        - [True Colors](http://www.memo.tv/portfolio/true-colors/)
        - [Gloomy Sunday](http://www.memo.tv/portfolio/gloomy-sunday/)
        - [We are made of star dust](http://www.memo.tv/portfolio/we-are-made-of-star-dust/)
        - [Learning to Dream](http://www.memo.tv/portfolio/learning-to-dream)
        - [Learning to Dream: Supergan!](http://www.memo.tv/portfolio/learning-to-dream-supergan/)
        - [Learning to See: Hello, World!](http://www.memo.tv/learning-to-see-hello-world/)
        - [Dirty Data](http://www.memo.tv/portfolio/dirty-data/)
        - [“Learning to See” studies](http://www.memo.tv/portfolio/learning-to-see-studies/)
    - An artificial neural network looks out onto the world, trying to make sense of what it’s seeing, in context of what it’s seen before. But it can only see through the filter of what it already knows, just like us. Because **we too, see things not as they are, but as we are**.
    - Learning to See is an ongoing series of works that use state-of-the-art machine learning algorithms to reflect on ourselves and how we make sense of the world. The picture we see in our conscious mind is not a mirror image of the outside world, but is a reconstruction based on our expectations and prior beliefs.
    - The work is part of a broader line of inquiry about self affirming cognitive biases, our inability to see the world from others’ point of view, and the resulting social polarization.
    - ## Description
    - The work is an interactive installation in which a number of neural networks analyse a live camera feed pointing at a table covered in everyday objects. Through a very tactile, hands-on experience, the audience can manipulate the objects on the table with their hands, and see corresponding scenery emerging on the display, in realtime, reinterpreted by the neural networks. Every 30 seconds the scene changes between different networks trained on five different datasets: (the four natural elements:) ocean & waves (representing ‘water’), clouds & sky (representing ‘air’), fire, flowers (representing earth, and life); and images from the Hubble Space telescope (representing the universe, cosmos, aether, void or God). The interaction can be a very short, quick, playful experience. Or the audience can spend hours meticulously crafting their perfect nebula, or shaping their favourite waves, or arranging a beautiful bouquet.
